{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing Back Propagation\n",
    "#For this recipe, we will show how to do TWO separate examples, a regression example, and a classification example\n",
    "#To illustrate how to do back propagation with TensorFlow, we start by loading the necessary libraries and resetting the computational graph.\n",
    "\n",
    "import matplotlib.pyplot  as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "#Create a Graph Session\n",
    "sess = tf.Session()\n",
    "\n",
    "#A Regression Example\n",
    "#We create a regression example as follows. The input data will be 100 random samples from a normal (mean of 1.0, stdev of 0.1). The target will be 100 constant values of 10.0.\n",
    "#We will fit the regression model:  x_data * A = target_values\n",
    "#Theoretically, we know that A should be equal to 10.0.\n",
    "#We start by creating the data and targets with their respective placholders\n",
    "\n",
    "x_vals = np.random.normal(1, 0.1 ,100)\n",
    "y_vals = np.repeat(10., 100)\n",
    "#https://blog.csdn.net/u010496337/article/details/50572866\n",
    "x_data = tf.placeholder(shape = [1], dtype = tf.float32)\n",
    "y_target = tf.placeholder(shape = [1], dtype = tf.float32)\n",
    "#print(y_target.get_shape())\n",
    "#(1,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create variable (one model parameter = A)\n",
    "A = tf.Variable(tf.random_normal(shape = [1]))\n",
    "#print(sess.run(A))\n",
    "#[-0.42807996]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add operation to graph\n",
    "my_output = tf.multiply(x_data, A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add L2 lpss operation to graph\n",
    "loss = tf.square(my_output - y_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Optimizer\n",
    "my_opt =  tf.train.GradientDescentOptimizer(0.02)\n",
    "train_step = my_opt.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #25A = [9.8952465]\n",
      "Loss = [0.8050664]\n",
      "Step #50A = [10.072941]\n",
      "Loss = [0.07206836]\n",
      "Step #75A = [10.008321]\n",
      "Loss = [0.18475758]\n",
      "Step #100A = [9.747149]\n",
      "Loss = [2.7233486]\n"
     ]
    }
   ],
   "source": [
    "#Running the Regression Graph\n",
    "#Run loop\n",
    "for i in range(100):\n",
    "    rand_index = np.random.choice(100)\n",
    "    rand_x = [x_vals[rand_index]]\n",
    "    rand_y = [y_vals[rand_index]]\n",
    "    sess.run(train_step, feed_dict = {x_data: rand_x, y_target: rand_y})\n",
    "    if (i + 1)%25 == 0:\n",
    "        print('Step #' + str(i +1) + 'A = ' + str(sess.run(A)))\n",
    "        print('Loss = ' + str(sess.run(loss, feed_dict = {x_data: rand_x, y_target:rand_y})))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification Example\n",
    "#For the classification example, we will create an x-sample made of two different normal distribution inputs, Normal(mean = -1, sd = 1) and Normal(mean = 3, sd = 1). For each of these the target will be the class 0 or 1 respectively.\n",
    "#The model will fit the binary classification: If sigmoid(x+A) < 0.5 then predict class 0, else class 1.\n",
    "#Theoretically, we know that A should take on the value of the negative average of the two means: -(mean1 + mean2)/2.\n",
    "#We start by resetting the computational graph:\n",
    "ops.reset_default_graph()\n",
    "#Create graph\n",
    "sess = tf.Session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vals = np.concatenate((np.random.normal(-1, 1, 50), np.random.normal(3,1,50)))\n",
    "#https://blog.csdn.net/zyl1042635242/article/details/43162031\n",
    "y_vals = np.concatenate((np.repeat(0., 50), np.repeat(1.,50)))\n",
    "x_data = tf.placeholder(shape = [1], dtype = tf.float32)\n",
    "y_target = tf.placeholder(shape = [1], dtype = tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create variable(one model parameter = A)\n",
    "A = tf.Variable(tf.random_normal(mean = 10, shape = [1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add operation to graph\n",
    "#Want to Create the operation sigmoid(x + A)\n",
    "#Note, the sogmoid() part is in the loss function\n",
    "my_output = tf.add(x_data, A)\n",
    "\n",
    "#Now we have to add another dimension to each(batch size of 1)\n",
    "my_output_expanded = tf.expand_dims(my_output, 0)\n",
    "y_target_expanded= tf.expand_dims(y_target, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add classification liss(cross entropy)\n",
    "xentropy = tf.nn.sigmoid_cross_entropy_with_logits(logits = my_output_expanded, labels = y_target_expanded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Optimizer\n",
    "my_opt = tf.train.GradientDescentOptimizer(0.05)\n",
    "train_step = my_opt.minimize(xentropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializw variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step # 200 A = [1.0972729]\n",
      "Loss = [[0.54000163]]\n",
      "Step # 400 A = [-0.37212175]\n",
      "Loss = [[0.2514137]]\n",
      "Step # 600 A = [-0.6854644]\n",
      "Loss = [[0.08447079]]\n",
      "Step # 800 A = [-0.7544683]\n",
      "Loss = [[0.19501132]]\n",
      "Step # 1000 A = [-0.89852136]\n",
      "Loss = [[0.13081811]]\n",
      "Step # 1200 A = [-0.982475]\n",
      "Loss = [[0.13166176]]\n",
      "Step # 1400 A = [-0.84348816]\n",
      "Loss = [[0.02613202]]\n"
     ]
    }
   ],
   "source": [
    "#Running the Classification Graph\n",
    "#Run loop\n",
    "for i in range(1400):\n",
    "    rand_index = np.random.choice(100)\n",
    "    rand_x = [x_vals[rand_index]]\n",
    "    rand_y = [y_vals[rand_index]]\n",
    "    \n",
    "    sess.run(train_step, feed_dict = {x_data:rand_x, y_target:rand_y})\n",
    "    if (i + 1) % 200 == 0:\n",
    "        print('Step # ' + str(i + 1) + ' A = ' + str(sess.run(A)))\n",
    "        print('Loss = ' + str(sess.run(xentropy, feed_dict = {x_data:rand_x, y_target:rand_y})))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
